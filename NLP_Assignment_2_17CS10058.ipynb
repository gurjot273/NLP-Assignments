{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_2_17CS10058.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zyJ25uz0kSaw"
      },
      "source": [
        "# Assignment 2 on Natural Language Processing\n",
        "\n",
        "### Date : 15th Sept, 2020\n",
        "\n",
        "### Instructor : Prof. Sudeshna Sarkar\n",
        "\n",
        "### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ao1nhg9RknmF"
      },
      "source": [
        "The central idea of this assignment is to make you familiar with programming in python and also the language modelling task of natural language processing using the python.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "stk58juYkzEr"
      },
      "source": [
        "**Dataset:** \n",
        "\n",
        " Use the text file provided along."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rT6byv49kdmo",
        "colab": {}
      },
      "source": [
        "# read dataset\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "data=\"\"\n",
        "with open(\"corpus.txt\") as f:\n",
        "  data=f.read()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SRGqKaDn1pJy"
      },
      "source": [
        "Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C1OtHn6B1oc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "58c2c223-166e-42fd-8bec-b2f0d42a3914"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # For tokenizers\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "sentences = data.split('\\n') # sentence split\n",
        "sentences = [sentence.lower() for sentence in sentences] # lowercase\n",
        "tokenizer = RegexpTokenizer(r'[^\\W_]+')\n",
        "processed_sentences = [tokenizer.tokenize(sentence) for sentence in sentences] # tokenizing sentences into sequences not containing punctuation and underscores"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YDL7yfpXkMRS"
      },
      "source": [
        "### Task: In this sub-task, you are expected to carry out the following tasks:\n",
        "\n",
        "1. **Create the following language models** on the training corpus: <br>\n",
        "    i.   Unigram <br>\n",
        "    ii.  Bigram <br>\n",
        "    iii. Trigram <br>\n",
        "    iv.  Fourgram <br>\n",
        "\n",
        "2. **List the top 5 bigrams, trigrams, four-grams (with and without Add-1 smoothing).**\n",
        "(Note: Please remove those which contain only articles, prepositions, determiners. For Example: “of the”, “in a”, etc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u3oIulBikPua",
        "colab": {}
      },
      "source": [
        "#Write code\n",
        "\n",
        "from nltk.util import ngrams\n",
        "unigrams=[]\n",
        "bigrams=[]\n",
        "trigrams=[]\n",
        "fourgrams=[]\n",
        "\n",
        "for content in processed_sentences:\n",
        "    unigrams.extend(content)\n",
        "    bigrams.extend(ngrams(content,2))\n",
        "    ##similar for trigrams and fourgrams\n",
        "    trigrams.extend(ngrams(content,3))\n",
        "    fourgrams.extend(ngrams(content,4))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vARsvSfynePr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "7aa36cb4-3546-475c-a567-f6f4ec63c033"
      },
      "source": [
        "#stopwords = code for downloading stop words through nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "#print top 10 unigrams, bigrams, trigrams, fourgrams without removing stopwords\n",
        "uni_fdist = nltk.FreqDist(unigrams)\n",
        "print(\"\\nWithout stopword removal, top 10 unigrams with their counts:\\n\",uni_fdist.most_common(10))\n",
        "\n",
        "bi_fdist = nltk.FreqDist(bigrams)\n",
        "print(\"\\nWithout stopword removal, top 10 bigrams with their counts:\\n\",bi_fdist.most_common(10))\n",
        "\n",
        "tri_fdist = nltk.FreqDist(trigrams)\n",
        "print(\"\\nWithout stopword removal, top 10 trigrams with their counts:\\n\",tri_fdist.most_common(10))\n",
        "\n",
        "four_fdist = nltk.FreqDist(fourgrams)\n",
        "print(\"\\nWithout stopword removal, top 10 fourgrams with their counts:\\n\",four_fdist.most_common(10))\n",
        "\n",
        "print(\" \\n############################################################\\n\")\n",
        "\n",
        "#print top 10 unigrams, bigrams, trigrams, fourgrams after removing stopwords\n",
        "uni_processed = [p for p in unigrams if p not in stopwords]\n",
        "uni_processed_fdist = nltk.FreqDist(uni_processed)\n",
        "print(\"\\nAfter stopword removal, top 10 unigrams with their counts:\\n\",uni_processed_fdist.most_common(10))\n",
        "\n",
        "bi_processed = [p for p in bigrams if (p[0] not in stopwords or p[1] not in stopwords)]\n",
        "bi_processed_fdist = nltk.FreqDist(bi_processed)\n",
        "print(\"\\nAfter stopword removal, top 10 bigrams with their counts:\\n\",bi_processed_fdist.most_common(10))\n",
        "\n",
        "tri_processed = [p for p in trigrams if (p[0] not in stopwords or p[1] not in stopwords or p[2] not in stopwords)]\n",
        "tri_processed_fdist = nltk.FreqDist(tri_processed)\n",
        "print(\"\\nAfter stopword removal, top 10 trigrams with their counts:\\n\",tri_processed_fdist.most_common(10))\n",
        "\n",
        "four_processed = [p for p in fourgrams if (p[0] not in stopwords or p[1] not in stopwords or p[2] not in stopwords or p[3] not in stopwords)]\n",
        "four_processed_fdist = nltk.FreqDist(four_processed)\n",
        "print(\"\\nAfter stopword removal, top 10 fourgrams with their counts:\\n\",four_processed_fdist.most_common(10))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "\n",
            "Without stopword removal, top 10 unigrams with their counts:\n",
            " [('the', 1643), ('and', 872), ('to', 729), ('a', 632), ('it', 595), ('she', 553), ('i', 545), ('of', 514), ('said', 462), ('you', 411)]\n",
            "\n",
            "Without stopword removal, top 10 bigrams with their counts:\n",
            " [(('said', 'the'), 209), (('of', 'the'), 133), (('said', 'alice'), 116), (('in', 'a'), 97), (('and', 'the'), 82), (('in', 'the'), 79), (('it', 'was'), 76), (('the', 'queen'), 72), (('to', 'the'), 69), (('the', 'king'), 62)]\n",
            "\n",
            "Without stopword removal, top 10 trigrams with their counts:\n",
            " [(('the', 'mock', 'turtle'), 53), (('i', 'don', 't'), 31), (('the', 'march', 'hare'), 30), (('said', 'the', 'king'), 29), (('the', 'white', 'rabbit'), 21), (('said', 'the', 'hatter'), 21), (('said', 'to', 'herself'), 19), (('said', 'the', 'mock'), 19), (('said', 'the', 'caterpillar'), 18), (('she', 'went', 'on'), 17)]\n",
            "\n",
            "Without stopword removal, top 10 fourgrams with their counts:\n",
            " [(('said', 'the', 'mock', 'turtle'), 19), (('she', 'said', 'to', 'herself'), 16), (('a', 'minute', 'or', 'two'), 11), (('you', 'won', 't', 'you'), 10), (('said', 'the', 'march', 'hare'), 8), (('will', 'you', 'won', 't'), 8), (('said', 'alice', 'in', 'a'), 7), (('i', 'don', 't', 'know'), 7), (('as', 'well', 'as', 'she'), 6), (('well', 'as', 'she', 'could'), 6)]\n",
            " \n",
            "############################################################\n",
            "\n",
            "\n",
            "After stopword removal, top 10 unigrams with their counts:\n",
            " [('said', 462), ('alice', 397), ('little', 128), ('one', 104), ('know', 87), ('like', 85), ('would', 83), ('went', 83), ('could', 77), ('queen', 75)]\n",
            "\n",
            "After stopword removal, top 10 bigrams with their counts:\n",
            " [(('said', 'the'), 209), (('said', 'alice'), 116), (('the', 'queen'), 72), (('the', 'king'), 62), (('a', 'little'), 59), (('mock', 'turtle'), 56), (('the', 'mock'), 53), (('the', 'gryphon'), 53), (('the', 'hatter'), 52), (('went', 'on'), 48)]\n",
            "\n",
            "After stopword removal, top 10 trigrams with their counts:\n",
            " [(('the', 'mock', 'turtle'), 53), (('the', 'march', 'hare'), 30), (('said', 'the', 'king'), 29), (('the', 'white', 'rabbit'), 21), (('said', 'the', 'hatter'), 21), (('said', 'to', 'herself'), 19), (('said', 'the', 'mock'), 19), (('said', 'the', 'caterpillar'), 18), (('she', 'went', 'on'), 17), (('she', 'said', 'to'), 17)]\n",
            "\n",
            "After stopword removal, top 10 fourgrams with their counts:\n",
            " [(('said', 'the', 'mock', 'turtle'), 19), (('she', 'said', 'to', 'herself'), 16), (('a', 'minute', 'or', 'two'), 11), (('said', 'the', 'march', 'hare'), 8), (('said', 'alice', 'in', 'a'), 7), (('i', 'don', 't', 'know'), 7), (('as', 'well', 'as', 'she'), 6), (('well', 'as', 'she', 'could'), 6), (('in', 'a', 'great', 'hurry'), 6), (('in', 'a', 'tone', 'of'), 6)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ioc1xNjmnim-"
      },
      "source": [
        "# Applying Smoothing\n",
        "\n",
        "\n",
        "Assume additional training data in which each possible N-gram occurs exactly once and adjust estimates.\n",
        "\n",
        "> ### $ Probability(ngram) = \\frac{Count(ngram)+1}{ N\\, +\\, V} $\n",
        "\n",
        "N: Total number of N-grams <br>\n",
        "V: Number of unique N-grams\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "grh4sO0Yns4V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "592254c4-9d7d-451c-c0ad-503035ec0f7e"
      },
      "source": [
        "#You are to perform Add-1 smoothing here:\n",
        "#write similar code for bigram, trigram and fourgrams\n",
        "\n",
        "#unigrams\n",
        "unique_unigrams = set(unigrams)\n",
        "N = len(unigrams)\n",
        "V = len(unique_unigrams)\n",
        "# uni_probability is dictionary for storing probability of each unigram in corpus\n",
        "uni_probability = {unigram: (uni_fdist[unigram]+1.0)/(N+V) for unigram in unique_unigrams}\n",
        "\n",
        "#bigrams\n",
        "unique_bigrams = set(bigrams)\n",
        "N = len(bigrams)\n",
        "V = len(unique_bigrams)\n",
        "# bi_probability is dictionary for storing probability of each bigram in corpus\n",
        "bi_probability = {bigram: (bi_fdist[bigram]+1.0)/(N+V) for bigram in unique_bigrams}\n",
        "\n",
        "#trigrams\n",
        "unique_trigrams = set(trigrams)\n",
        "N = len(trigrams)\n",
        "V = len(unique_trigrams)\n",
        "# tri_probability is dictionary for storing probability of each trigram in corpus\n",
        "tri_probability = {trigram: (tri_fdist[trigram]+1.0)/(N+V) for trigram in unique_trigrams}\n",
        "\n",
        "#fourgrams\n",
        "unique_fourgrams = set(fourgrams)\n",
        "N = len(fourgrams)\n",
        "V = len(unique_fourgrams)\n",
        "# four_probability is dictionary for storing probability of each fourgram in corpus\n",
        "four_probability = {fourgram: (four_fdist[fourgram]+1.0)/(N+V) for fourgram in unique_fourgrams}\n",
        "\n",
        "#Print top 10 unigram, bigram, trigram, fourgram after smoothing\n",
        "from operator import itemgetter \n",
        "print(\"\\nAfter smoothing, top 10 unigrams with their probabilities:\\n\",sorted(uni_probability.items(), key = itemgetter(1), reverse = True)[:10])\n",
        "print(\"\\nAfter smoothing, top 10 bigrams with their probabilities:\\n\",sorted(bi_probability.items(), key = itemgetter(1), reverse = True)[:10])\n",
        "print(\"\\nAfter smoothing, top 10 trigrams with their probabilities:\\n\",sorted(tri_probability.items(), key = itemgetter(1), reverse = True)[:10])\n",
        "print(\"\\nAfter smoothing, top 10 fourgrams with their probabilities:\\n\",sorted(four_probability.items(), key = itemgetter(1), reverse = True)[:10])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "After smoothing, top 10 unigrams with their probabilities:\n",
            " [('the', 0.054975922953451044), ('and', 0.029193418940609953), ('to', 0.024411449973247727), ('a', 0.021167736757624397), ('it', 0.019930444087747457), ('she', 0.018525949705724985), ('i', 0.018258426966292134), ('of', 0.017221776350989836), ('said', 0.015482878544676297), ('you', 0.013777421080791868)]\n",
            "\n",
            "After smoothing, top 10 bigrams with their probabilities:\n",
            " [(('said', 'the'), 0.005233644859813084), (('of', 'the'), 0.003339563862928349), (('said', 'alice'), 0.0029158878504672897), (('in', 'a'), 0.002442367601246106), (('and', 'the'), 0.0020685358255451715), (('in', 'the'), 0.0019937694704049843), (('it', 'was'), 0.0019190031152647976), (('the', 'queen'), 0.0018193146417445484), (('to', 'the'), 0.0017445482866043614), (('the', 'king'), 0.0015700934579439252)]\n",
            "\n",
            "After smoothing, top 10 trigrams with their probabilities:\n",
            " [(('the', 'mock', 'turtle'), 0.0011510177981455825), (('i', 'don', 't'), 0.0006820846211233081), (('the', 'march', 'hare'), 0.0006607694767132047), (('said', 'the', 'king'), 0.0006394543323031014), (('said', 'the', 'hatter'), 0.0004689331770222743), (('the', 'white', 'rabbit'), 0.0004689331770222743), (('said', 'to', 'herself'), 0.00042630288820206754), (('said', 'the', 'mock'), 0.00042630288820206754), (('said', 'the', 'caterpillar'), 0.0004049877437919642), (('said', 'the', 'gryphon'), 0.0003836725993818608)]\n",
            "\n",
            "After smoothing, top 10 fourgrams with their probabilities:\n",
            " [(('said', 'the', 'mock', 'turtle'), 0.0004183225266680611), (('she', 'said', 'to', 'herself'), 0.0003555741476678519), (('a', 'minute', 'or', 'two'), 0.00025099351600083665), (('you', 'won', 't', 'you'), 0.0002300773896674336), (('will', 'you', 'won', 't'), 0.00018824513700062747), (('said', 'the', 'march', 'hare'), 0.00018824513700062747), (('said', 'alice', 'in', 'a'), 0.00016732901066722442), (('i', 'don', 't', 'know'), 0.00016732901066722442), (('t', 'you', 'will', 'you'), 0.00014641288433382137), (('the', 'moral', 'of', 'that'), 0.00014641288433382137)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k0GL40mQmmt4"
      },
      "source": [
        "### Predict the next word using statistical language modelling\n",
        "\n",
        "Using the above bigram, trigram, and fourgram models that you just experimented with, **predict the next word(top 5 probable) given the previous n(=2, 3, 4)-grams** for the sentences below.\n",
        "\n",
        "For str1, str2, you are to predict the next  2 possible word sequences using your trained smoothed models. <br> \n",
        "For example, for the string 'He looked very' the answers can be as below: \n",
        ">     (1) 'He looked very' *anxiouxly* \n",
        ">     (2) 'He looked very' *uncomfortable* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MBWKo5_Fmnbg",
        "colab": {}
      },
      "source": [
        "str1 = 'after that alice said the'\n",
        "str2 = 'alice felt so desperate that she was'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ext_nVn2mvZt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "11a6b04e-da34-4ec8-9178-dcf52f01e693"
      },
      "source": [
        "#write code\n",
        "#predict returns the top k next words given the string str and n using n-gram model\n",
        "def predict(str,n,k):\n",
        "  n_probability={} #dictionary which stores probability of ngram\n",
        "  if n==2:\n",
        "    n_probability=bi_probability\n",
        "  elif n==3:\n",
        "    n_probability=tri_probability\n",
        "  else:\n",
        "    n_probability=four_probability\n",
        "  words = tokenizer.tokenize(str) # tokenizing the string\n",
        "  # the word having the highest conditional probability given the last n-1 words will be same as the n-gram with the highest probability and whose first n-1 words match the last n-1 words of str\n",
        "  probability={ngram: n_probability[ngram] for ngram in n_probability.keys() if list(ngram)[:-1]==words[-n+1:]} # dictionary storing the probability of the n-grams whose first n-1 words match the last n-1 tokens of the string str\n",
        "  if len(probability)==0: # no such n-gram\n",
        "    return [\"NULL\"]\n",
        "  top_five_probability=sorted(probability.items(), key = itemgetter(1), reverse = True)[:k] # sort based on probability and get top 5 n-grams\n",
        "  top_five_words=[x[0][-1] for x in top_five_probability] # get last word from n-gram\n",
        "  return top_five_words\n",
        "\n",
        "print(\"\\nFor str1, top 5 probable next words using bigram model:\",predict(str1,2,5))\n",
        "print(\"\\nFor str1, top 5 probable next words using trigram model:\",predict(str1,3,5))\n",
        "print(\"\\nFor str1, top 5 probable next words using fourgram model:\",predict(str1,4,5))\n",
        "\n",
        "print(\"\\nFor str2, top 5 probable next words using bigram model:\",predict(str2,2,5))\n",
        "print(\"\\nFor str2, top 5 probable next words using trigram model:\",predict(str2,3,5))\n",
        "print(\"\\nFor str2, top 5 probable next words using fourgram model:\",predict(str2,4,5))\n",
        "\n",
        "print(\"\\n########################################################\\n\")\n",
        "\n",
        "# Using greedy approach to predict next 2 words for str1\n",
        "next_word=predict(str1,2,1)[0]\n",
        "next_to_next_word=predict(str1+' '+next_word,2,1)[0]\n",
        "print(\"\\nFor str1, the next 2 word sequences using bigram model are:\",next_word+' '+next_to_next_word)\n",
        "\n",
        "next_word=predict(str1,3,1)[0]\n",
        "next_to_next_word=predict(str1+' '+next_word,3,1)[0]\n",
        "print(\"\\nFor str1, the next 2 word sequences using trigram model are:\",next_word+' '+next_to_next_word)\n",
        "\n",
        "next_word=predict(str1,4,1)[0]\n",
        "next_to_next_word=predict(str1+' '+next_word,4,1)[0]\n",
        "print(\"\\nFor str1, the next 2 word sequences using fourgram model are:\",next_word+' '+next_to_next_word)\n",
        "\n",
        "# Using greedy approach to predict next 2 words for str2\n",
        "next_word=predict(str2,2,1)[0]\n",
        "next_to_next_word=predict(str2+' '+next_word,2,1)[0]\n",
        "print(\"\\nFor str1, the next 2 word sequences using bigram model are:\",next_word+' '+next_to_next_word)\n",
        "\n",
        "next_word=predict(str2,3,1)[0]\n",
        "next_to_next_word=predict(str2+' '+next_word,3,1)[0]\n",
        "print(\"\\nFor str1, the next 2 word sequences using trigram model are:\",next_word+' '+next_to_next_word)\n",
        "\n",
        "next_word=predict(str2,4,1)[0]\n",
        "next_to_next_word=predict(str2+' '+next_word,4,1)[0]\n",
        "print(\"\\nFor str1, the next 2 word sequences using fourgram model are:\",next_word+' '+next_to_next_word)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "For str1, top 5 probable next words using bigram model: ['queen', 'king', 'gryphon', 'mock', 'hatter']\n",
            "\n",
            "For str1, top 5 probable next words using trigram model: ['king', 'hatter', 'mock', 'caterpillar', 'gryphon']\n",
            "\n",
            "For str1, top 5 probable next words using fourgram model: ['NULL']\n",
            "\n",
            "For str2, top 5 probable next words using bigram model: ['a', 'the', 'not', 'going', 'that']\n",
            "\n",
            "For str2, top 5 probable next words using trigram model: ['now', 'quite', 'a', 'beginning', 'walking']\n",
            "\n",
            "For str2, top 5 probable next words using fourgram model: ['now', 'dozing', 'quite', 'losing', 'in']\n",
            "\n",
            "########################################################\n",
            "\n",
            "\n",
            "For str1, the next 2 word sequences using bigram model are: queen and\n",
            "\n",
            "For str1, the next 2 word sequences using trigram model are: king and\n",
            "\n",
            "For str1, the next 2 word sequences using fourgram model are: NULL NULL\n",
            "\n",
            "For str1, the next 2 word sequences using bigram model are: a little\n",
            "\n",
            "For str1, the next 2 word sequences using trigram model are: now about\n",
            "\n",
            "For str1, the next 2 word sequences using fourgram model are: now about\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}