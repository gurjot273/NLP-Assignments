{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_3_17CS10058.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyJ25uz0kSaw"
      },
      "source": [
        "# Assignment 3 on Natural Language Processing\n",
        "\n",
        "## Date : 30th Sept, 2020\n",
        "\n",
        "### Instructor : Prof. Sudeshna Sarkar\n",
        "\n",
        "### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao1nhg9RknmF"
      },
      "source": [
        "The central idea of this assignment is to use Naive Bayes classifier and LSTM based classifier and compare the models by accuracy on IMDB dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONM5Q4SCe9Mr"
      },
      "source": [
        "Please submit with outputs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElRkQElWUMjG"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhHRim2AUm4z"
      },
      "source": [
        "#Load the IMDB dataset. You can load it using pandas as dataframe\n",
        "df = pd.read_csv(\"IMDB Dataset.csv\")"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK_Hn2f6VMP7"
      },
      "source": [
        "# Preprocessing\n",
        "PrePrecessing that needs to be done on lower cased corpus\n",
        "\n",
        "1. Remove html tags\n",
        "2. Remove URLS\n",
        "3. Remove non alphanumeric character\n",
        "4. Remove Stopwords\n",
        "5. Perform stemming and lemmatization\n",
        "\n",
        "You can use regex from re. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B5lHZPsVOXv",
        "outputId": "d0129273-6a20-47e6-c1e1-db21b011297a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import nltk\n",
        "import nltk.tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stopwords = set(stopwords.words('english'))\n",
        "reviews = df['review'].values\n",
        "labels = df['sentiment'].values\n",
        "# lowercase\n",
        "reviews = [review.lower() for review in reviews]\n",
        "# remove html tags\n",
        "reviews = [re.sub(r'<.*?>', ' ', review) for review in reviews]\n",
        "# remove url\n",
        "reviews = [re.sub(r'http[s]?://\\S+', ' ', review) for review in reviews]\n",
        "reviews = [re.sub(r'www\\.\\S+', ' ', review) for review in reviews]\n",
        "reviews = [review.split(' ') for review in reviews]\n",
        "# remove non alphanumeric characters\n",
        "reviews = [[re.sub(r'[^a-zA-Z0-9]','', word) for word in review] for review in reviews]\n",
        "# remove stopwords\n",
        "reviews = [[word for word in review if word!='' and word not in stopwords] for review in reviews]\n",
        "# lemmatization\n",
        "reviews = [[lemmatizer.lemmatize(word,pos='v') for word in review] for review in reviews]\n",
        "reviews = [[word for word in review if word!=''] for review in reviews]\n",
        "reviews = [' '.join(words) for words in reviews]\n",
        "df['review'] = reviews"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyaSkfcvYGXk",
        "outputId": "b1f4979b-b338-425a-d48e-7d5948f560a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Print Statistics of Data like avg length of sentence , proposition of data w.r.t class labels\n",
        "# find total number of words in corpus\n",
        "total_words = sum([len(review.split(' ')) for review in reviews])\n",
        "# distinct_words = set()\n",
        "# find distinct words in corpus\n",
        "# for i, review in enumerate(reviews):\n",
        "#   distinct_words = distinct_words.union(set(review.split(' ')))\n",
        "# num_distinct_words = len(distinct_words)\n",
        "# find max_length of sentence in corpus\n",
        "max_length = max([len(review.split(' ')) for review in reviews])\n",
        "num_sentences = len(reviews)\n",
        "# print(\"Number of distinct words in corpus is:\", num_distinct_words)\n",
        "print(\"Average length of sentence is:\", total_words/num_sentences)\n",
        "print(\"Maximum length of sentence is:\", max_length)\n",
        "print(\"Proportion of data with positive labels is:\",np.sum(labels=='positive')/len(labels))\n",
        "print(\"Proportion of data with negative labels is:\",np.sum(labels=='negative')/len(labels))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of distinct words in corpus is: 153417\n",
            "Average length of sentence is: 120.23092\n",
            "Maximum length of sentence is: 1437\n",
            "Proportion of data with positive labels is: 0.5\n",
            "Proportion of data with negative labels is: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FkJ-e2pUwun"
      },
      "source": [
        "# Naive Bayes classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVq-mN28U_J4"
      },
      "source": [
        "# get reviews column from df\n",
        "reviews = df['review'].values\n",
        "\n",
        "# get labels column from df\n",
        "labels = df['sentiment'].values"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljo5NquhXTXr",
        "outputId": "0def3a48-2a18-458b-accc-520cd6b4c2a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Use label encoder to encode labels. Convert to 0/1\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(labels)\n",
        "\n",
        "print(encoder.classes_)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['negative' 'positive']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzG-C_EVWWET"
      },
      "source": [
        "# Split the data into train and test (80% - 20%). \n",
        "# Use stratify in train_test_split so that both train and test have similar ratio of positive and negative samples.\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(reviews, encoded_labels, test_size=0.20, stratify=encoded_labels, random_state=0)\n",
        "\n",
        "# train_sentences, test_sentences, train_labels, test_labels"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz1YdsSkiWCX"
      },
      "source": [
        "Here there are two approaches possible for building vocabulary for the naive Bayes.\n",
        "1. Take the whole data (train + test) to build the vocab. In this way while testing there is no word which will be out of vocabulary.\n",
        "2. Take the train data to build vocab. In this case, some words from the test set may not be in vocab and hence one needs to perform smoothing so that one the probability term is not zero.\n",
        " \n",
        "You are supposed to go by the 2nd approach.\n",
        " \n",
        "Also building vocab by taking all words in the train set is memory intensive, hence you are required to build vocab by choosing the top 2000 - 3000 frequent words in the training corpus.\n",
        "\n",
        "> $ P(x_i | w_j) = \\frac{ N_{x_i,w_j}\\, +\\, \\alpha }{ N_{w_j}\\, +\\, \\alpha*d} $\n",
        "\n",
        "\n",
        "$N_{x_i,w_j}$ : Number of times feature $x_i$ appears in samples of class $w_j$\n",
        "\n",
        "$N_{w_j}$ : Total count of features in class $w_j$\n",
        "\n",
        "$\\alpha$ : Parameter for additive smoothing. Here consider $\\alpha$ = 1\n",
        "\n",
        "$d$ : Dimentionality of the feature vector  $x = [x_1,x_2,...,x_d]$. In our case its the vocab size.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cllNfGmUr77"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Use Count vectorizer to get frequency of the words\n",
        "'''\n",
        "max_features parameter : If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
        "vec = CountVectorizer(max_features = 3000)\n",
        "X = vec.fit_transform(Sentence_list)\n",
        "'''\n",
        "vectorizer = CountVectorizer(max_features = 3000)\n",
        "# fitting on train_sentences and converting  to train_features array\n",
        "train_features = vectorizer.fit_transform(train_sentences).toarray()\n",
        "# get the vocabulary\n",
        "vocab = vectorizer.get_feature_names()"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzRvPjWaWUnm"
      },
      "source": [
        "# Use laplace smoothing for words in test set not present in vocab of train set\n",
        "\n",
        "# Solution provided in next cell in conditional_probability function while building the model using dictionary vocab_map"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE7pxWIYW1z0"
      },
      "source": [
        "# Build the model. Don't use the model from sklearn\n",
        "num_classes = 2\n",
        "d = len(vocab)\n",
        "\n",
        "# probability of pos and neg classes\n",
        "neg_class_prob = np.sum(train_labels==0)/len(train_labels)\n",
        "pos_class_prob = np.sum(train_labels==1)/len(train_labels)\n",
        "\n",
        "# class_word_counts stores total count of a particular word in a particular class\n",
        "class_word_counts = np.zeros((num_classes,d))\n",
        "# class_counts stores total count of words in a particular class\n",
        "class_counts = np.zeros(num_classes)\n",
        "\n",
        "for i in range(num_classes):\n",
        "  class_mask = (train_labels==i) # find examples belonging to ith class\n",
        "  class_word_counts[i] = np.sum(train_features[class_mask],axis=0) # find total count of particular word in a class\n",
        "\n",
        "class_counts = np.sum(class_word_counts,axis=1) # find total num words for each class\n",
        "\n",
        "# dictionary storing indices of words in vocabulary\n",
        "vocab_map = {word:i for i, word in enumerate(vocab)}\n",
        "\n",
        "# Use laplace smoothing for words in test set not present in vocab of train set\n",
        "# Finds the smoothed conditional probability of a word given its class\n",
        "def conditional_probability(word, label):\n",
        "  n=0 # if word not in vocabulary\n",
        "  if word in vocab_map:\n",
        "    n=class_word_counts[label,vocab_map[word]] # if word in vocabulary, count of word in class\n",
        "  return (n+1)/(class_counts[label]+d)\n",
        "\n",
        "# predicts label for sentences based on Naive Bayes model constructed above\n",
        "def predict(sentence):\n",
        "  words = sentence.split(' ')\n",
        "  # sum of log probabilities is used instead of product so that product of small number does not go below precision\n",
        "  log_neg_prob = np.sum([np.log(conditional_probability(word,0)) for word in words]) # adding log(p(word|class)) for neg class\n",
        "  log_pos_prob = np.sum([np.log(conditional_probability(word,1)) for word in words]) # adding log(p(word|class)) for pos class\n",
        "  log_neg_prob += np.log(neg_class_prob) # adding log(p(class)) for neg class\n",
        "  log_pos_prob += np.log(pos_class_prob) # adding log(p(class)) for pos class\n",
        "  # find the more probable class\n",
        "  if log_pos_prob>=log_neg_prob:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtQSl1zvW4DD",
        "outputId": "3bf886e2-d787-477a-83f8-cadb8bae49b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Test the model on test set and report Accuracy\n",
        "predicted_test_labels = [predict(sentence) for sentence in test_sentences]\n",
        "# finding accuracy on test set\n",
        "test_accuracy = np.mean(predicted_test_labels==test_labels)\n",
        "print(\"Accuracy on test set is:\",test_accuracy)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test set is: 0.8444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlNql0acU7sa"
      },
      "source": [
        "# *LSTM* based Classifier\n",
        "\n",
        "Use the above train and test splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkqnvbUOXoN0"
      },
      "source": [
        "# Hyperparameters of the model\n",
        "# as the vocabulary size is 153417 which is too large, we take the 3000 most frequent words as vocab\n",
        "vocab_size = 3000 # choose based on statistics\n",
        "oov_tok = '<OOK>'\n",
        "embedding_dim = 100\n",
        "# as average length of sentence is 120, max_length is taken as 150 and the rest of words are ignored\n",
        "max_length = 150 # choose based on statistics, for example 150 to 200\n",
        "padding_type='post'\n",
        "trunc_type='post'"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeycEg9nZAOF"
      },
      "source": [
        "# tokenize sentences\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# convert train dataset to sequence and pad sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
        "\n",
        "# convert Test dataset to sequence and pad sequences\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtw3w895ZP39",
        "outputId": "7045eefb-4629-4cee-daec-ef95e87ec7d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# model initialization\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
        "    keras.layers.Dense(24, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# compile model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "model.summary()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 150, 100)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, 128)               84480     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 24)                3096      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1)                 25        \n",
            "=================================================================\n",
            "Total params: 387,601\n",
            "Trainable params: 387,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skmaDJMnZTzc",
        "outputId": "1c3ba251-3150-4f87-b68e-eff43a0ef94b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "num_epochs = 5\n",
        "history = model.fit(train_padded, train_labels, \n",
        "                    epochs=num_epochs, verbose=1, \n",
        "                    validation_split=0.1)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.3805 - accuracy: 0.8298WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_test_batch_end` time: 0.0121s). Check your callbacks.\n",
            "1125/1125 [==============================] - 87s 77ms/step - loss: 0.3805 - accuracy: 0.8298 - val_loss: 0.3306 - val_accuracy: 0.8677\n",
            "Epoch 2/5\n",
            "1125/1125 [==============================] - 86s 77ms/step - loss: 0.2679 - accuracy: 0.8936 - val_loss: 0.3150 - val_accuracy: 0.8648\n",
            "Epoch 3/5\n",
            "1125/1125 [==============================] - 86s 76ms/step - loss: 0.2337 - accuracy: 0.9071 - val_loss: 0.3030 - val_accuracy: 0.8785\n",
            "Epoch 4/5\n",
            "1125/1125 [==============================] - 86s 77ms/step - loss: 0.1993 - accuracy: 0.9240 - val_loss: 0.3283 - val_accuracy: 0.8620\n",
            "Epoch 5/5\n",
            "1125/1125 [==============================] - 86s 77ms/step - loss: 0.1701 - accuracy: 0.9374 - val_loss: 0.3480 - val_accuracy: 0.8723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjEhWEr5Zq7M",
        "outputId": "61e301c4-adbf-4ac1-915a-c9f8c0adfb4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Calculate accuracy on Test data\n",
        "'''\n",
        "prediction = model.predict(test_padded)\n",
        "\n",
        "'''\n",
        "# Get probabilities\n",
        "prediction_prob = model.predict(test_padded)\n",
        "\n",
        "# Get labels based on probability 1 if p>= 0.5 else 0\n",
        "predicted_test_labels = [1 if prob>=0.5 else 0 for prob in prediction_prob]\n",
        "\n",
        "# Accuracy : one can use classification_report from sklearn\n",
        "test_accuracy = np.mean(predicted_test_labels==test_labels)\n",
        "print(\"Accuracy on test set is\", test_accuracy)\n",
        "print(\"--------------------------\")\n",
        "print(classification_report(test_labels,predicted_test_labels))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test set is 0.8703\n",
            "--------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.83      0.87      5000\n",
            "           1       0.84      0.91      0.87      5000\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIICV-ySOYL0"
      },
      "source": [
        "## Get predictions for random examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2RmfNL3OYL0",
        "outputId": "6c6325d9-e733-4428-d5e3-1844fdfa284e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# reviews on which we need to predict\n",
        "sentence = [\"The movie was very touching and heart whelming\", \n",
        "            \"I have never seen a terrible movie like this\", \n",
        "            \"the movie plot is terrible but it had good acting\"]\n",
        "\n",
        "# convert to a sequence\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "\n",
        "# pad the sequence\n",
        "padded = pad_sequences(sequences, padding='post', maxlen=max_length)\n",
        "\n",
        "# Get probabilities\n",
        "prediction_prob = model.predict(padded)\n",
        "print(prediction_prob)\n",
        "\n",
        "# Get labels based on probability 1 if p>= 0.5 else 0\n",
        "predicted_labels = [1 if prob>=0.5 else 0 for prob in prediction_prob]\n",
        "print(\"Predicted labels:\",predicted_labels)\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.69801337]\n",
            " [0.1681131 ]\n",
            " [0.12915443]]\n",
            "Predicted labels: [1, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}